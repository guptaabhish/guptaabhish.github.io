<!--#include file="head.html" -->

<div id="content">

My primary area of research is communication in parallel computing. This
includes predicting and understanding the behavior of HPC networks for scalable
application execution, topology aware mapping to reduce communication time, and
developing scalable algorithms for communication intensive operations.  I am
also interested in parallel discrete event simulations, interoperation of
parallel languages, such as Charm++, MPI, UPC, etc., and development of adaptive
runtime systems. In my free time, I like to think of collective algorithms for
different network topologies and parallel machine learning algorithms. 

<br>
<br>

<b> Key Projects </b>
<ul >

<li>
<b>Simulation based analysis of HPC networks</b>
<ul id=project_list>
<li> Proposed and designed TraceR, a fast packet-level parallel network 
simulator for understanding and accurately predicting behavior of applications 
on extreme scale networks.
<li> Created Damselfly, a functional model that estimates the flow of 
traffic on Dragonfly networks, to find the best routing scheme and job 
placement policy for scenarios with  multi-job executions.
</ul>
<br>

<li>
<b>Supervised learning for predicting performance of communication-bound 
applications</b>
<ul id=project_list>
<li> Deployed ensemble methods, Extremely Randomized Trees and Gradient
Boosted Regression Trees, to correctly predict performance of HPC applications.
<li> Identified a subset of network features, viz. average bytes and above
average stalls on links, as the most important factors that impact the
communication.
</ul>
<br>

<li>
<b>OpenAtom: Enabling AIMD at scale</b>
<ul id=project_list>
<li> Developed generalized topology-aware mapping schemes for Cray XE6/XK7
and IBM Blue Gene/Q, which resulted in up to 50\% reduction in execution time of
OpenAtom.
<li> Designed and implemented a 2D-decomposition based message-driven FFT 
library that improved the strong scaling of a $436$-molecule system to $65,536$ 
cores.
</ul>
<br>

<li>
<b>Interoperation of Charm++ and MPI</b>
<ul id=project_list>
<li> Created a framework to enable coexistence of Charm++ and MPI in 
production applications.
<li> Demonstrated the productivity and performance benefits of
interoperation via several example use cases, such as  parallel FFTW in NAMD, 
Charm++'s sorting library in Chombo, MPI-IO in EpiSimdemics, etc.
</ul>
<br>

<li>
<b>Charm++ features development</b>
<ul style="list-style-type:desc">
<li> Augmented the Charm++ runtime system with Partition framework, which
enables execution of several loosely-connected Charm++ instances within a single
application execution. This capability is currently used by NAMD for
replica-exchange algorithm and Charm++ for soft-error fault tolerance.
<li> Active contributor to various levels of Charm++ stack, including the
communication layer (PAMI, VERBS, and MPI), load balancing framework, and 
Adaptive MPI.
</ul>
</ul >

<b>Career at IBM</b>- Prior to joining UIUC, I was a member of the High
Performance Computing Group, managed by Dr. Yogish Sabharwal, in IBM
Research-India. While at IBM, I worked on HPCC benchmark optimization for IBM's
Blue Gene series, developing algorithms for MPI Collectives and scalability of
scientific and real life applications.  

</div>

<!--#include file="trail.html" -->
